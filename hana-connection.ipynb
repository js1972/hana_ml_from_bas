{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66d6b5e",
   "metadata": {},
   "source": [
    "# HANA ML & HANA AI Experiments\n",
    "\n",
    "## Setup a connection to SAP Datasphere (SAP HANA Cloud database) and use HANA_ML\n",
    "\n",
    "A notebook to show how to connect to the Datasphere tenant in your SAP Business Data Cloud formation (or standalone) and experiment with its machine learning capabilities.\n",
    "> Note that this applies to any SAP HANA Cloud system and not just Datasphere, given that SAP HANA Cloud is the database used by SAP Datasphere.\n",
    "\n",
    "- How to connect to the sql schema exposed by your Datasphere Space\n",
    "- Using the [hana_ml python library](https://pypi.org/project/hana-ml/) *APL* to build machine learning models directly in Datasphere without copying the data (federated)\n",
    "See the [hana_ml samples on github](https://github.com/SAP-samples/hana-ml-samples/tree/main) for lots of examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae5869a",
   "metadata": {},
   "source": [
    "### Pre-requisites\n",
    "\n",
    "First you need to create a db user for your Datasphere Space (you will require *space admin* privileges).  \n",
    "This db user needs to have `Enable Automated Predictive Library (APL) and Predictive Analysis Library (PAL)` set to TRUE when you create it. This option is only avilable when Dataspheres underlying SAP HANA Cloud db has the script server enabled in *System Configuration -> tenant configuration* (requires a minimum 3 vCPU system sizing).\n",
    "\n",
    "e.g.\n",
    "\n",
    "I have a space called `PLAYPEN_JASON_SCOTT` (of type HANA memory & disk).\n",
    "\n",
    "In the Space settings -> Create a db user. I have specified a user suffix of `MLUSER` so the user schema is `PLAYPEN_JASON_SCOTT#MLUSER`.\n",
    "\n",
    "Next we will get some sample data to use for the rest of this notebook:\n",
    "1. Clone repo: https://github.com/SAP-samples/hana-apl-apis-runtimes to your local computer\n",
    "2. Open the HANA Database Explorer and connect to your exposed schema from your Datasphere Space\n",
    "3. Right-click over \"Catalog\" and choose to \"Import Catalog Objects\" -> select the file: `dataForHANACloud.tar.gz`\n",
    "4. Ensure to overwrite the schema name to your own exposed schema `PLAYPEN_JASON_SCOTT#MLUSER` as thats the only schema you have full access to.\n",
    "\n",
    "With our Space exposing an SQL schema and user and some sample data loaded we are ready to build some ML models...\n",
    "\n",
    "Finally, [Setup a *Python Dev Space* in **SAP Business Application Studio**](https://github.com/SAP-samples/hana-ml-py-codejam/blob/main/exercises/00-setup/setup-bas.md).\n",
    "> Alternatively you can work on a local machine with vscode (probably the best way is to use docker and a vscode devcontainer for **Anaconda**. This devcontainer comes fully pre-configured for python notebooks and typical data science tasks)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c7c05b",
   "metadata": {},
   "source": [
    "### Install dependencies\n",
    "Install `hana_ml` and a few other misc. dependencies that can be handy when working in python notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d7823",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --require-virtualenv -U 'ipykernel' 'ipython' 'nbformat' 'nbconvert'\n",
    "!python -m pip install --require-virtualenv -U 'hana-ml' 'ipywidgets' 'graphviz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfce43fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip list --not-required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609ca9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hana_ml\n",
    "print (f\"SAP HANA Client for Python: {hana_ml.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb90706",
   "metadata": {},
   "source": [
    "### Setup connection to HANA Cloud DB exposed from Datasphere\n",
    "This next python cell will ask you to enter the connection details for your exposed Datasphere SQL schema. By doing it this way we don't need to worry about saving passwords into the notebook (*the outputs below will not be captured in git*).  \n",
    "In a production scenario, however, you should use proper secrets management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b62388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hana_ml data frame is what enables us to work with remote data in the HANA db\n",
    "# When the collect method is called it copies the data locally into a pandas dataframe.\n",
    "from hana_ml import dataframe\n",
    "from hana_ml.dataframe import ConnectionContext\n",
    "\n",
    "host_address = input(\"Enter SAP HANA Cloud host name:\")\n",
    "hdb_user = input(\"Enter User Name :\")\n",
    "hdb_password = input(\"Enter Password :\")\n",
    "\n",
    "hana_port = 443\n",
    "hana_encrypt = True #for HANA Cloud\n",
    "\n",
    "# Establish connection\n",
    "conn = dataframe.ConnectionContext(address = host_address,\n",
    "                                   port = hana_port, \n",
    "                                   user = hdb_user, \n",
    "                                   password = hdb_password, \n",
    "                                   encrypt = hana_encrypt,\n",
    "                                   sslValidateCertificate = 'false')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c57fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.connection.isconnected()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a7320",
   "metadata": {},
   "source": [
    "## 📊 What’s a DataFrame?\n",
    "\n",
    "A **DataFrame** is like a **table in memory** — rows and columns of data you can easily filter, sort, group, or analyze.  \n",
    "\n",
    "- Each **column** has a name and a data type.  \n",
    "- Each **row** is an observation or record.  \n",
    "- It’s fast, flexible, and perfect for working with structured data.  \n",
    "\n",
    "Think of it as a spreadsheet… but programmable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e651d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "    \"Age\": [25, 30, 35]\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7236c0e7",
   "metadata": {},
   "source": [
    "## 🪴 What’s a `hana_ml` DataFrame?\n",
    "\n",
    "A **`hana_ml` DataFrame** is a **virtual reference to data stored in SAP HANA**, not data in local memory.  \n",
    "\n",
    "- It **doesn’t copy** the data — it pushes operations down to the HANA database.  \n",
    "- You can filter, join, aggregate, and apply ML directly **inside HANA**.  \n",
    "- It looks and feels like a regular DataFrame, but it’s **backed by SQL**.  \n",
    "\n",
    "Think of it as a “smart SQL view” that behaves like a Python DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71fcba8",
   "metadata": {},
   "source": [
    "Let's look at one of the sample data tables inside SAP Datasphere/SAP HANA Cloud.\n",
    "> Note the `collect()` method on the hana_ml dataframe. This copies the data to the local machine as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d5c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_cmd = 'SELECT * FROM AUTO_CLAIMS_FRAUD ORDER BY CLAIM_ID'\n",
    "hdf_train = dataframe.DataFrame(conn, sql_cmd)\n",
    "hdf_train.head(6).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43482fa2",
   "metadata": {},
   "source": [
    "Now that we are connected to the exposed sql schema for our Datasphere Space we can use the full power of HANA Cloud and its machine learning libraries. Reasons to use the ML capabilities of HANA include:\n",
    "- Models may not be available in SAP Databricks (such as AutoML for Classification and Regression)\n",
    "- No standard BDC Data Products for the dataset\n",
    "- Models can be exposed as CAP artefacts for use in transactional applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a379d3b6",
   "metadata": {},
   "source": [
    "# 1. We will now use the APL library in HANA to train a machine learning model for fraud detection\n",
    "For this fraud detection classification task we will use a gradient boosting binary classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db423845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.algorithms.apl.gradient_boosting_classification import GradientBoostingBinaryClassifier\n",
    "apl_model = GradientBoostingBinaryClassifier(variable_auto_selection = True)\n",
    "apl_model.fit(hdf_train, label='IS_FRAUD', key='CLAIM_ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7848854",
   "metadata": {},
   "source": [
    "## Show an auto-generated report on the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0087b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.visualizers.unified_report import UnifiedReport\n",
    "UnifiedReport(apl_model).build().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5632bc5",
   "metadata": {},
   "source": [
    "What variables were excluded from the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e945b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apl_model.get_debrief_report('ClassificationRegression_VariablesExclusion').collect()\n",
    "df = df[['Variable', 'Reason For Exclusion']]\n",
    "df.style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8490bc51",
   "metadata": {},
   "source": [
    "Model performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d40059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter = \"\\\"Partition\\\" = 'Validation' and \\\"Indicator\\\" in ('AUC','F1 Score','Cohen''s kappa')\"\n",
    "df = apl_model.get_debrief_report('ClassificationRegression_Performance').filter(my_filter).collect()\n",
    "df.drop('Oid', axis=1, inplace=True)\n",
    "format_dict = {'Value':'{:,.3f}'}\n",
    "df.style.format(format_dict).hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46799ab6",
   "metadata": {},
   "source": [
    "Binary target statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ae187",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter = \"\\\"Partition\\\"='Validation'\"\n",
    "df = apl_model.get_debrief_report('BinaryTarget_Statistics').filter(my_filter).collect()\n",
    "df.drop('Oid', axis=1, inplace=True)\n",
    "format_dict = {'% Positive Weight':'{:,.1f}%', '% Negative Weight':'{:,.1f}%', 'Weight':'{:,.0f}'}\n",
    "df.style.format(format_dict).hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec2359",
   "metadata": {},
   "source": [
    "### ⚠️ Class Imbalance in Fraud Detection\n",
    "\n",
    "Our validation data shows **only ~10% positive cases (`IS_FRAUD = 1`)** and ~90% negatives.  \n",
    "This is a **highly imbalanced dataset**, which means:\n",
    "\n",
    "- A default **0.5 decision threshold** will bias predictions toward the majority class (`No Fraud`).\n",
    "- Standard accuracy isn’t a good performance measure here.\n",
    "- Precision, recall, and AUC are more meaningful.\n",
    "\n",
    "👉 **What to do about it:**\n",
    "- Use **predicted probabilities** from the model instead of the default label.  \n",
    "- **Tune the decision threshold** (e.g., using F1 or cost-based optimization).  \n",
    "- Consider resampling strategies or class weights if needed.  \n",
    "- Focus on **precision–recall trade-offs**, not just accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada76ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.model_storage import ModelStorage\n",
    "model_storage = ModelStorage(connection_context=conn, schema=hdb_user)\n",
    "apl_model.name = 'My Fraud Model'  \n",
    "model_storage.save_model(model=apl_model, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc62593",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_storage.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0be549",
   "metadata": {},
   "source": [
    "## Load the saved model and make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a621f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.model_storage import ModelStorage\n",
    "model_storage = ModelStorage(connection_context=conn, schema=hdb_user)\n",
    "apl_model = model_storage.load_model(name='My Fraud Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47adc74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_tables = apl_model.get_model_info()\n",
    "\n",
    "for i, hdf in enumerate(info_tables, 1):\n",
    "    print(f\"\\n=== Model Info Table #{i} ===\")\n",
    "    display(hdf.head(20).collect())   # pull a small sample to pandas for display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ac24dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apl_model.get_summary().filter(\"KEY in ('ModelVariableCount', 'ModelSelectedVariableCount', 'ModelRecordCount', 'ModelBuildDate')\").collect()\n",
    "df['KEY'] = df['KEY'].str.replace('Model', '').str.replace('Selected', 'Selected ')\n",
    "df['KEY'] = df['KEY'].str.replace('Count', ' Count').str.replace('Date', ' Date')\n",
    "df = df[['KEY','VALUE']]\n",
    "df.columns = ['Property', 'Value']\n",
    "df.style.hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077c2620",
   "metadata": {},
   "source": [
    "## Make Predictions on New Claims\n",
    "Load some new claims data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01068b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_new = conn.table('AUTO_CLAIMS_NEW', schema=hdb_user)\n",
    "hdf_new.head(3).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14eebe9",
   "metadata": {},
   "source": [
    "Predict on the new data (The extra_applyout_settings tells SAP HANA APL to enrich the prediction output with additional columns like probabilities, decision values, and reason codes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acccfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "apl_model.set_params(extra_applyout_settings=\n",
    "{ 'APL/ApplyExtraMode': 'Advanced Apply Settings', \n",
    "  'APL/ApplyPredictedValue': 'true', \n",
    "  'APL/ApplyProbability': 'true', \n",
    "  'APL/ApplyDecision': 'true', \n",
    "  'APL/ApplyReasonCode/TopCount': '3', \n",
    "  'APL/ApplyReasonCode/ShowStrengthValue': 'false', \n",
    "  'APL/ApplyReasonCode/ShowStrengthIndicator': 'false' }\n",
    ")\n",
    "df = apl_model.predict(hdf_new).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6378ea6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dict = {'PREDICTED': 'Target Predicted', \n",
    "        'gb_score_IS_FRAUD': 'Score', \n",
    "        'gb_proba_IS_FRAUD': 'Probability'}\n",
    "df.rename(columns=col_dict, inplace=True)\n",
    "df.columns = [hdr.replace(\"gb_\", \"\") for hdr in df.columns]\n",
    "format_dict = {'Probability': '{:,.1%}','Score':'{0:,.2f}'}\n",
    "df.head(7).style.format(format_dict).hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c21877",
   "metadata": {},
   "source": [
    "# 2. Lets do an APL Regression scenario now..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507a0476",
   "metadata": {},
   "source": [
    "Load sample census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc31d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_cmd = 'SELECT * FROM CENSUS order by \"id\"'\n",
    "hdf_train = dataframe.DataFrame(conn, sql_cmd)\n",
    "hdf_train.head(6).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee6baca",
   "metadata": {},
   "source": [
    "## Fit with APL Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367fea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hana_ml.algorithms.apl.gradient_boosting_regression import GradientBoostingRegressor\n",
    "apl_model = GradientBoostingRegressor(eval_metric='MAE', variable_auto_selection = True)\n",
    "apl_model.fit(hdf_train, label='age', key='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914eb59",
   "metadata": {},
   "source": [
    "## Model reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036730c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = apl_model.get_debrief_report('ClassificationRegression_VariablesContribution').collect()\n",
    "df = df.sort_values(by=['Rank'])\n",
    "df.drop({'Oid','Method','Rank'}, axis=1, inplace=True)\n",
    "df.drop(df[df.Contribution <=0].index, inplace=True)\n",
    "format_dict = {'Contribution':'{:,.2%}','Cumulative':'{:,.2%}'}\n",
    "df.style.format(format_dict).hide(axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c515e47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_filter = \"\\\"Partition\\\" = 'Validation' and \\\"Indicator\\\" in ('MAPE','RMSE')\"\n",
    "df = apl_model.get_debrief_report('ClassificationRegression_Performance').filter(my_filter).collect()\n",
    "df.drop('Oid', axis=1, inplace=True)\n",
    "format_dict = {'Value':'{:,.3f}'}\n",
    "df.style.format(format_dict).hide(axis='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c786cf51",
   "metadata": {},
   "source": [
    "## Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2882c814",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_cmd = 'SELECT * FROM CENSUS LIMIT 100'\n",
    "hdf_apply = dataframe.DataFrame(conn, sql_cmd)\n",
    "df = apl_model.predict(hdf_apply).collect()\n",
    "df.columns = ['id', 'Actual','Prediction']\n",
    "df.head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cd426",
   "metadata": {},
   "source": [
    "# What next...\n",
    "\n",
    "We have trained classicial machine learning models directly inside the hana database and we never had to replicate the data. We can now iterate opver this process to understand the data better and work toawrd better results ofr your business case.\n",
    "\n",
    "The models are saved inside hana and we can make use of them in a number of ways... some of which include:\n",
    "\n",
    "1. Execute the model (inference) on fresh data via SQL (db procedure call). This can then be scheduled via a Task Chain and can provide a nice view over the new data and inference results\n",
    "1. Execute the model from a CAP application running on BTP (hana_ml can be iused to generate the required CAP artefacts)\n",
    "1. Create a python app to execute the model with fresh data as a web API and execute it from SAP AI Core / BTP Kyma / BTP Cloud Foundry environments\n",
    "\n",
    "> Investigate and experiment with the companion package [hana_ai](https://pypi.org/project/hana-ai/) where you can use plain english text to generate your hana_ml code (this uses GenAI models via SAP AI Core)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
